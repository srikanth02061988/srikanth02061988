import subprocess
import sys
import json
import psycopg2 import sql

#global variable declaration for constant values
host_name = 'npe'
database_name =''
user_name =''
password = ''
port_no = ''
meta_tbl=''
workflow_database = "None"
workflow_container =""
job_name ="None"
last_load_timestamp = 170

def connect_to_database(dbname):
    try:
        connection = psycopg2.connect(database=dbname, user=user_name,password=password,
                                      host=host_name,port=port_no)
        return  connection
    except psycopg2.Error as e:
        print(f"unable to connect to the database, Error: {e}")
        return None

def retrieve_metadata(connection, meta_tbl):
    global workflow_database
    global workflow_container
    global last_load_timestamp
    global job_name
    uniqueid = ""
    try:
        cursor=connection.cursor()

        # sql query to selecting meta table data
        query = sql.SQL("SELECT json_data from {} ORDER BY id ASC ").format(
            sql.Identifier(meta_tbl)
        )
        cursor.execute(query)

        # fetching all rows
        rows = cursor.fetchall()
        print(rows)

        for row in rows:
            print(row)
            jar_path = row['jar_path']
            main_class = row['main_class']
            databrciksMode = row['databricksMode']
            cosmosConfName = row['cosmosConfName']
            secretScope = row['secretScope']
            existing_cluster_id = row['existing_cluster_id']

            print(jar_path)

            # selecting config database of param information database
            database =  connect_to_database(config_db)
            print(database)

            # selecting meta data information
            metadb = connect_to_database(metadata_database)

            # selecting config data container


            metadb = connect_to_database(meta_db)

            # selecting cofig data container
            container =

            # Function to get the last processed timestamp
            last_load_timestamp = get_last_processed_timesamp(metadb, timestamp_container)
            print("last_load_timestamp:", last_load_timestamp)

            data_items = get_uniqueid_info(last_load_timestamp, metadb, config_meta_container)
            print("The unique data", data_items)

            # Iterate over each uniqueID in db table code
            update_last_processed_timestamp(update_timestamp, metadb,timestamp_container)

            for ids in data_items:
                update_timestamp = ids[1]
                # Creating new cursor for each uid iteration
                with database.cursor() as cursor:
                    # SQL query
                    query = sql.SQL("SELECT json_data FROM {} WHERE uid LIKE {}").format(
                        sql.Identifier(timestamp_container),
                        sql.Literal(uniqueid + '%')
                    )
                    print(query)
                    cursor.execute(query)
                    # Fetching all rows
                    rows = cursor.fetchall()
                    print(rows)
                    for row in rows:
                        json_data = row['json_data']
                        # accessing key and value from the bronze data
                        if 'bronze' in json_data:
                            bronze_values = json_data['bronze']
                            print("Bronze Values:", bronze_values)
                            for items in bronze_values:
                                job_name = items['workflowName']
                                taskFilePath = items['filename']
                                configFolder = items['filePath']
                                args = ["databricksMode={}".format(databrciksMode),
                                        "cosmosConfName={}".format(cosmosConfName),
                                        "secretScope={}".format(secretScope),
                                        "taskFilePath={}".format(taskFilePath),
                                        "configFolder={}".format(configFolder)]
                                # calling job_schedule_config function
                                job_schedule_config(existing_cluster_id,main_class,args,jar_path,
                                                    bronze_schedule, timezone)


                        if 'silder' in json_data:
                            silver_values = json_data['silver']
                            print("silver_values", silver_values)


            # the cursor is closing
            cursor.close()


def job_schedule_config(existing_cluster_id,main_class,args,jar_path,bronze_schedule, timezone):
    print("the jobname", job_name)
    job_config = {
        "name": job_name,
        "existing_cluster_id": existing_cluster_id,
        "spark_jar_task": {
            "main_class_name": main_class,
            "parameters": args,
            "jar_url": jar_path
        },
        "schedule": {
            "quartz_cron_expression": job_schedule,
            "timezone_id": timezone
        }
    }
    create_and_run_databricks_job(job_config)

def create_and_run_databricks_job(job_info):
    config_data_json = json.dumps(job_info)



def get_uniqueid_info(last_load_timestamp, metadb, config_meta_container):
    # selecting meta data information in meta table

    try:
        cursor=metadb.cursor()

        # sql query to selecting meta table data
        query = sql.SQL("SELECT json_data from {} ORDER BY id ASC ").format(
            sql.Identifier(meta_tbl)
        )
        cursor.execute(query)

        # fetching all rows
        rows = cursor.fetchall()
        print(rows)

        for row in rows:
            print(row)

def update_last_processed_timestamp(update_timestamp, database, timestamp_container):
    # connect to the timestamp database
    timestamp_db = connect_to_database(database)
    try:
        with timestamp_db.cursor() as cursor:
            # query to fetching latest timestamp table
            query_select = f"SELECT * FROM {timestamp_container}"
            cursor.execute(query_select)
            rows = cursor.fetchall()

            if rows:
                for row in rows:
                    # updating the existing row
                    query_update = f"UPDATE {timestamp_container} SET latest_timestamp = %s WHERE id = %s"
                    cursor.execute(query_update, (update_timestamp, row['id']))
                    print("The latest timestamp updated in DB")
            else:
                # creating a new row if it doesn't exist
                query_insert = f"INSERT INTO {timestamp_container} (id, latest_timestamp) VALUES (%s, %s)"
                cursor.execute(query_insert, ('uniquid_document_id', update_timestamp))
                print("A new row with the latest timestamp has been inserted into DB")

        # committing the changes
        timestamp_db.commit()

    finally:
        # db connection close
        timestamp_db.close()



if __name__ == "__main__":
    # connecting to the postgreSQL database
    connection = connect_to_database(database_name)

    if connection:
        # Retrieving the constant table data
        retrieve_metadata(connection,meta_tbl)

